\section{Описание программной реализации}

Программная реализация была выполнена на языке C с использованием стандарта \texttt{C11}. Для распараллеливания решения задачи на разных вычислительных узлах использовалась библиотека \texttt{MPI}. Для распараллеливания в рамках одного вычислительного узла (содержащего несколько вычислительных ядер) была использована технология OpenMP. Исходный код содержит собственную библиотеку для более удобной работы с динамическими массивами в языке C \texttt{array.h}.

В разработанной программе перед решением поставленной задачи инициализируется декартова топология, разбивается прямоугольник $\Pi$ и инициализируется сетка и используемые массивы.

После чего начинается итерационный процесс, останавливающийся при выполнении ранее указанного неравенства (\ref{eq:stop}). На каждой итерации каждый процесс получает и отправляет своим соседям свои граничные строки и столбцы (функция \texttt{neighbors\_exchange}), а затем использует эти значения. Происходит следующее число обменов:
\begin{enumerate}[label=\arabic*)]
  \item \textbf{0}, если процесс граничный в топологии,
  \item \textbf{2}, если процесс угловой,
  \item \textbf{3}, если процесс боковой,
  \item \textbf{4}, если процесс внутренний.
\end{enumerate}

При вычислении коэффициентов $\alpha$ и $\tau$ каждый процесс получает и суммирует вычисленные значения со значениями с других процессов.

После окончания итерации процессы вычисляют значение нормы по указанной в предыдущем разделе формуле, затем обмениваются вычисленными значениями и каждый процесс хранит максимальное, после чего проверяется критерий завершения итерационного процесса (\ref{eq:stop}).

Затем в процессе с ранком 0 происходит сбор решения от всех процессов в единый массив (функция \texttt{make\_solution}).

\subsection{OpenMP}

По условиям задания было необходимо реализовать версию программы с использованием \texttt{OpenMP}. Это было реализовано следующим образом.

Ограничение процессорных ядер до трех вместо четырех было сделано с использованием функции \texttt{omp\_set\_num\_threads(3);} в начале функции \texttt{main}. Затем для каждого цикла, выполняющего операции над векторами, была использована директива \texttt{\#pragma omp parallel for}, которая распределяет итерации между параллельными потоками. Для каждого цикла, выполняющего операции над переменной \texttt{varname} (например, вычисление суммы), была использована директива \texttt{\#pragma omp parallel for reduction(+:varname)}, которая также распределяет итерации между параллельными потоками, но при этом хранит свою локальную копию переменной, после чего все локальные копии сливаются в исходную переменную.

\subsection{Сохранение и загрузка состояния}

Так как для программ студентов время расчетов на ПВС <<IBM Blue Gene/P>> сильно ограничено (300 секунд), то был написан механизм сохранения и загрузки состояния расчетов.

К аргументам коммандной строки программы добавлен третий параметр, который может принимать следующие значения:
\begin{enumerate}[label=\arabic*)]
  \item \textbf{save}~-- при использовании этого параметра программа сохранит расчеты через число итераций, указанное в четвертом параметре;
  \item \textbf{loadsave}~-- при использовании этого параметра программа загрузит сохраненные ранее расчеты, а затем сохранит новые расчеты через число итераций, указанное в четвертом параметре;
  \item \textbf{load}~-- при использовании этого параметра программа загрузит сохраненные ранее расчеты.
\end{enumerate}

\clearpage
